{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T10:47:26.673402Z",
     "start_time": "2025-04-10T10:47:25.862710Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "# numpy 2.0 for bitwise count operations\n",
    "import numpy as np\n",
    "import operator\n",
    "import itertools \n",
    "import re"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions:\n",
    "\n",
    "1. Place the paths to the dataset you want to extract DCs from in \"dataset\".\n",
    "2. Set the number of rows to use from the dataset. It must be a multiple of 8, but a power of 2 is preferable.\n",
    "3. Remember that DCs work with tuple pairs. Both time and memory usage scale quadratically. Using 2000 (2¹¹) tuples usually works well.\n",
    "4. This results in analyzing 4 million tuple pairs. In the paper, we used 16000 (2¹⁴) tuples, but it takes much longer as it needs to process 300 million tuple pairs.\n",
    "*Note*:\n",
    "Simply specifying the required dataset should be enough for it to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classes to parse and represent Datasets and DCs as defined in the literature."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T10:47:32.605200Z",
     "start_time": "2025-04-10T10:47:32.564484Z"
    }
   },
   "source": [
    "class Operator:\n",
    "    def __init__(self,func,expFunc) -> None:\n",
    "        self.func=func\n",
    "        self.expFunc=expFunc\n",
    "        self.neg=None\n",
    "        self.imp=None\n",
    "    def __call__(self,a,b):\n",
    "        return self.func(a,b)\n",
    "    def negate(self):\n",
    "        return Operator(operator.invert(self.func))\n",
    "    def expected(self,c1,c2):\n",
    "        return self.expFunc(c1,c2)\n",
    "    def __repr__(self) -> str:\n",
    "        return revopmap[self]\n",
    "    def __eq__(self, other: object) -> bool:\n",
    "        if isinstance(other, Operator):\n",
    "            return self.func==other.func\n",
    "        return False\n",
    "    def __hash__(self):\n",
    "        fields=(self.func)\n",
    "        hash_value = hash(fields)\n",
    "        return hash_value\n",
    "\n",
    "def eqExp(l,r):\n",
    "    n=sum(l)**2\n",
    "    return np.sum(l**2)/n\n",
    "       \n",
    "\n",
    "eq=Operator(operator.eq,eqExp)\n",
    "\n",
    "def neExp(l,r):\n",
    "    n=sum(l)\n",
    "    return 1-np.sum(l**2)/n**2\n",
    "ne=Operator(operator.ne,neExp)\n",
    "\n",
    "def geExp(l,r):\n",
    "    n=sum(l)\n",
    "    cumFreq=np.cumsum(l)\n",
    "    return np.sum(l*(cumFreq))/n**2\n",
    "ge=Operator(operator.ge,geExp)\n",
    "\n",
    "def leExp(l,r):\n",
    "    n=sum(l)\n",
    "    cumFreq=np.cumsum(l)\n",
    "    return np.sum(l*(n-cumFreq+l))/n**2\n",
    "le=Operator(operator.le,leExp)\n",
    "\n",
    "def gtExp(l,r):\n",
    "    n=sum(l)\n",
    "    cumFreq=np.cumsum(l)\n",
    "    return np.sum(l*(cumFreq-l))/n**2\n",
    "gt=Operator(operator.gt,gtExp)\n",
    "\n",
    "def ltExp(l,r):\n",
    "    n=sum(l)\n",
    "    cumFreq=np.cumsum(l)\n",
    "    return np.sum(l*(n-cumFreq))/n**2\n",
    "lt=Operator(operator.lt,ltExp)\n",
    "operatorMap={\n",
    "    \"EQUAL\":eq,\n",
    "    \"UNEQUAL\":ne,\n",
    "    \"LESS_EQUAL\":le,\n",
    "    \"GREATER_EQUAL\":ge,\n",
    "    \"LESS\":lt,\n",
    "    \"GREATER\":gt\n",
    "}\n",
    "\n",
    "opmap={\"==\":eq,\"<>\":ne,\">=\":ge,\"<=\":le,\">\":gt,\"<\":lt}\n",
    "revopmap={y:x for x,y in opmap.items()}\n",
    "\n",
    "eq.neg=ne\n",
    "ne.neg=eq\n",
    "gt.neg=le\n",
    "le.neg=gt\n",
    "lt.neg=ge\n",
    "ge.neg=lt\n",
    "\n",
    "eq.imp=[ge,le,eq]\n",
    "ne.imp=[ne]\n",
    "gt.imp=[gt,ge,ne]\n",
    "lt.imp=[lt,le,ne]\n",
    "ge.imp=[ge]\n",
    "le.imp=[le]\n",
    "\n",
    "\n",
    "#wrong implications to avoid DCs of the sort > -> !=\n",
    "eq.imp=[ge,le,eq]\n",
    "ne.imp=[ne,lt,gt]\n",
    "gt.imp=[gt,ge]\n",
    "lt.imp=[lt,le]\n",
    "ge.imp=[ge]\n",
    "le.imp=[le]\n",
    "\n",
    "\n",
    "class Predicate:\n",
    "    def __init__(self,l:str,op:Operator,r:str) -> None:\n",
    "        self.l=l\n",
    "        self.r=r\n",
    "        self.op=op\n",
    "        self.exp=None\n",
    "    def __repr__(self) -> str:\n",
    "        return 't0.'+self.l +' '+self.op.__repr__()+' t1.'+self.r+''\n",
    "    def __hash__(self):\n",
    "        fields=(self.l,self.r)\n",
    "        hash_value = hash(fields)\n",
    "        return hash_value\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, Predicate):\n",
    "            sFields=(self.l,self.op,self.r)\n",
    "            oFields=(other.l,other.op,other.r)\n",
    "            return sFields==oFields\n",
    "        return False\n",
    "    def impliesPred(self,other):\n",
    "        #True if predicate being false implies other being false\n",
    "        return self.l==other.l and self.r==other.r and other.op.neg in self.op.neg.imp\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self,file,**args):\n",
    "        self.columns=pd.read_csv(file,nrows=0).columns\n",
    "        self.header=[re.match(r'([^\\(\\)]*)(?:\\(| )([^\\(\\)]*)\\)?',col) for col in self.columns]\n",
    "        self.names=[match[1] for match in self.header]\n",
    "        typeMap={'String':str,'Integer':float,'Int':float,'Double':float,'int':float,'str':str,'float':float}\n",
    "        self.types={col:typeMap[match[2]] for col,match in zip(self.columns,self.header)}\n",
    "        \n",
    "        self.df=pd.read_csv(file,**args,dtype=self.types)\n",
    "        for i,col in enumerate(self.columns):\n",
    "            self.df[col]=self.df[col].astype(self.types[col])\n",
    "        \n",
    "    def randRows(self,n):\n",
    "        ids=np.random.randint(0,len(self.df),n)\n",
    "        return self.df.iloc[ids]\n",
    "    def randFields(self,n):\n",
    "        return pd.DataFrame({col:dfs[col].iloc[list(np.random.randint(0,len(dfs),n))].values for dfs in [self.df] for col in dfs.columns})\n",
    "\n",
    "    def buildPLIs(self):\n",
    "        self.PLI= {col:self.df.groupby(by=col).groups for col in self.df}\n",
    "        self.PLILen={col:np.array([len(self.PLI[col][v])for v in self.PLI[col]]) for col in self.df}\n",
    "        self.vals={col:np.array([v for v in self.PLI[col]]) for col in self.df}\n",
    "    def shuffle(self):\n",
    "        self.df=self.randFields(len(self.df))\n",
    "\n",
    "    def buildPreds(self):\n",
    "        self.preds=[]\n",
    "        self.predMap={}\n",
    "        self.colPreds=[]\n",
    "        self.predCols=[]\n",
    "        \n",
    "        for col in self.columns:\n",
    "            ops=[eq,ne] if self.types[col] ==str else [eq,ne,gt,ge,lt,le]\n",
    "            self.colPreds.append([]) \n",
    "            for op in ops:\n",
    "                pred=Predicate(col,op,col)\n",
    "                self.predMap[(col,op,col)]=len(self.preds)\n",
    "                self.predCols.append(len(self.colPreds))\n",
    "                self.colPreds[-1].append(len(self.preds))\n",
    "                self.preds.append(pred)\n",
    "                \n",
    "    def buildEvi(self):\n",
    "        n=len(self.df)\n",
    "        m=len(self.preds)\n",
    "        self.eviSize=n*(n-1)\n",
    "\n",
    "        self.evi=[None]*m\n",
    "        self.predProbs=[None]*m\n",
    "        \n",
    "        for p in range(m):\n",
    "            pred=self.preds[p]\n",
    "            col=self.df[pred.l]\n",
    "            evis=[]\n",
    "            for i in range(n):\n",
    "                c1=col.iloc[i]\n",
    "                c2=col.iloc[i+1:n]\n",
    "                evis.append(pred.op(c1,c2))\n",
    "                c2=col.iloc[:i]\n",
    "                evis.append(pred.op(c1,c2))\n",
    "\n",
    "            allTPs=np.concatenate(evis)\n",
    "            self.evi[p]=np.packbits(allTPs,axis=0,bitorder='little')\n",
    "            self.predProbs[p]=allTPs.sum()/(n*(n-1))*2\n",
    "        self.sortedPreds=sorted(range(len(self.predProbs)),key=lambda i:self.predProbs[i])\n",
    "                \n",
    "class DenialConstraint:\n",
    "    def __init__(self,preds) -> None:\n",
    "        self.preds=preds\n",
    "        \n",
    "    def __sub__(self,other):\n",
    "        return DenialConstraint(self.preds-other.preds)\n",
    "    def __le__(self,other):\n",
    "        other:DenialConstraint=other\n",
    "        return all([ any([p==pp for pp in other.preds]) for p in self.preds])\n",
    "\n",
    "    def __eq__(self, value: object) -> bool:\n",
    "        return self<=value and value<=self\n",
    "    def __repr__(self) -> str:\n",
    "        return \"¬(\"+\" ^ \".join([pred.__repr__() for pred in self.preds])+\")\"\n",
    "    \n",
    "class DenialConstraintSet:\n",
    "    def __init__(self,path,dataset,dss,algorithm) -> None:        \n",
    "        self.predMap={}\n",
    "        self.preds=[]\n",
    "        self.dss=dss\n",
    "        opmap={\"==\":eq,\"<>\":ne,\">=\":ge,\"<=\":le,\">\":gt,\"<\":lt}\n",
    "        def getPred(c1,op,c2):\n",
    "            op=opmap[op]\n",
    "            return dss.predMap[(c1,op,c2)]\n",
    "        \n",
    "        self.DCs=[]\n",
    "        \n",
    "        with open(path) as f:\n",
    "            for line in f:\n",
    "                line=line.strip()[2:-1] #strip !(...)\n",
    "                preds=line.split('^')\n",
    "                regex=r't0\\.'+dataset+'\\.csv\\.([^=><]*)(==|<>|>=|<=|>|<)t1\\.'+dataset+'\\.csv\\.([^=><]*)'\n",
    "                if algorithm in ['ADCMiner','FastADC','ours']:\n",
    "                    regex=r't0\\.([^=><]*) (==|<>|>=|<=|>|<) t1\\.([^=><]*)'\n",
    "                preds = [getPred(*re.match(regex,pred.strip()).groups()) for pred in preds]\n",
    "                self.DCs.append(preds)\n",
    "\n",
    "    def buildGraph(self):\n",
    "        self.root=[{},None]\n",
    "        for dc in self.DCs:\n",
    "            node=self.root\n",
    "            for pred in sorted(dc):\n",
    "                if pred not in node[0]:\n",
    "                    node[0][pred]=[{},None]\n",
    "                node=node[0][pred]\n",
    "            node[1]=dc\n",
    "\n",
    "    def getReduced(self):\n",
    "       \n",
    "        notImplied=[True]*len(self.DCs)\n",
    "\n",
    "        def impliesDC(dc1,dc2):\n",
    "            return all([any([self.dss.preds[pred].impliesPred(self.dss.preds[otherpred]) for otherpred in dc2]) for pred in dc1])\n",
    "\n",
    "        for i,dc1 in enumerate(self.DCs):\n",
    "            for j,dc2 in enumerate(self.DCs):\n",
    "                if impliesDC(dc1,dc2):\n",
    "                    if impliesDC(dc2,dc1):\n",
    "                        notImplied[j]=notImplied[j] and j<=i\n",
    "                    else:\n",
    "                        notImplied[j]=False\n",
    "        return [dc for i,dc in enumerate(self.DCs) if notImplied[i]]\n",
    "        "
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T10:48:07.088293Z",
     "start_time": "2025-04-10T10:47:33.226219Z"
    }
   },
   "source": [
    "#Path to dataset for which  to discover DCs\n",
    "dataset=\"tax500k\"\n",
    "#Number of rows of the dataset to use\n",
    "rowCount=1<<11\n",
    "\n",
    "#Load dataset\n",
    "ds=Dataset(dataset+\".csv\",nrows=rowCount,encoding='unicode_escape')\n",
    "\n",
    "# Build Position List Indexes (PLIs)\n",
    "# These hold, for every unique element in each column, the rows that have that value in the respective column.\n",
    "# Allow to find tuple pairs that satisfy a certain predicate quickly. Ex: To find a 2 tuples on the same State, pick any \n",
    "# value in the PLI of column state, and choose two tuples from that group. They will both have the same State by definition.\n",
    "ds.buildPLIs()\n",
    "\n",
    "# Builds the predicates list. Ex: If there is a column \"State\", generates possible predicates \"t_x.State=t_y.State\" and \"t_x.State != t_y.State\".\n",
    "ds.buildPreds()\n",
    "\n",
    "# Builds the evidence set. Takes a long time.\n",
    "# The evidence set contains, for each pair of tuples, which predicates they fulfill.\n",
    "ds.buildEvi()"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We present a novel approach to determine the validity of a DC. Current methods use a very broad DC Validity definition that leads to discovery algorithms generating thousands of meaningless Denial Constraints. We show how by only keeping those DCs valid under our more restrictive definition we obtain entirely true, high-quality constraints of the data.\n",
    "\n",
    "We present a redefinition of the set of rules that qualify a DC as valid, not a DC discovery algorithm. Thus, the naive way to apply this contribution is by checking which elements among the vast space of sets of predicates constitute valid DCs. This is inefficient, but enough to showcase the significant improvement in the quality of the discovered DCs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first cell computes the number of tuple pairs that satisfy each set of predicates with up to **\"depth\"** elements. There are almost never more than 3 predicates in valid DCs, so if you want it to run much faster, just set **depth = 3**. We use **depth = 3** for completeness."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T10:55:23.515007Z",
     "start_time": "2025-04-10T10:48:54.580033Z"
    }
   },
   "source": [
    "#This utility function lists all subsets of a given set.\n",
    "def powerset(iterable):\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    s = list(iterable)\n",
    "    return itertools.chain.from_iterable(itertools.combinations(s, r) for r in range(len(s)))\n",
    "\n",
    "\n",
    "#Max size of predicate sets to search for\n",
    "depth=4\n",
    "DCCounts={}\n",
    "counts={}\n",
    "\n",
    "#Recursive function that performs a DFS search across the space of predicate sets up to length 4.\n",
    "def search(preds,cols,x):\n",
    "    #For every visited set of predicates, compute the proportion of tuple pairs it satisfies and store it.\n",
    "    counts[preds]=np.bitwise_count(x).sum()\n",
    "    #Stop the search if we are at max depth\n",
    "    if len(preds)>=depth:\n",
    "            return\n",
    "    #Otherwise, continue the DFS by exploring all predicate sets reachable from the current one by adding a new predicate.\n",
    "    for pred in ds.sortedPreds:\n",
    "            #We do not attempt to add predicates over the same column. This generates trivial DCs. Ex: State=State and State !=State.\n",
    "            ncol=ds.predCols[pred]\n",
    "            if ncol in cols:\n",
    "                continue\n",
    "            #This is a predicate set we want to visit. Explore it only if it has not been visited already.\n",
    "            npreds=preds|{pred}\n",
    "            ncols=cols|{ncol}\n",
    "            if npreds in counts:\n",
    "                continue\n",
    "            #This is an unvisited predicate set we want to visit. Filter the tuple pairs so as to keep those that satisfy the new set of predicates.\n",
    "            newx=np.bitwise_and(x,ds.evi[pred])\n",
    "            #Recursive call\n",
    "            search(npreds,ncols,newx)\n",
    "\n",
    "#Begin search:\n",
    "search(frozenset()  #Begin with an empty set of predicates\n",
    "       ,frozenset(),#Begin with no columns having predicates on the current set, as it is empty\n",
    "       np.full((ds.eviSize//8,),255,dtype=np.uint8))#Begin with all tuple pairs fulfilling the predicate set, as there is no predicate to not fulfill. This gets filtered as predicates are added during the DFS.\n",
    "\n",
    "DCCounts=counts"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell uses the previous information to determine which DCs are statistically significant, in the sense that the behavior of the predicates is determined by other predicates in the DC. This is our contribution, and by ensuring no DC is valid without representing some statistically significant relationship we guarantee discovered DCs correspond to a true relationship of the data.\n",
    "\n",
    "Let {A,B,C} be a set of predicates of a DC. We reject the DC if p(A|B C)=p(A|B) or p(A|C) or p(A). If this is the case, there is no relationship between ABC that is not captured by one of the subsets, so this set of predicates does not correspond to a valid DC, as some predicate has been added without it being part of the data constraint.\n",
    "\n",
    "We model the conditionals by means of a Type IV logistic distribution, and determine them to be different when it would be statistically implausible for the distributions to have the same population value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T14:26:16.617961Z",
     "start_time": "2025-02-27T14:26:10.557685Z"
    }
   },
   "outputs": [],
   "source": [
    "#Memoization for the digamma (y1) and trigamma (y2) functions used to compute mean and variance of the probability distribution we use.\n",
    "yconst=0.5772156649\n",
    "def y1():\n",
    "    mem=[-yconst]\n",
    "    def f(n):\n",
    "        while len(mem)<n:\n",
    "            mem.append(mem[-1]+1/len(mem))\n",
    "        return mem[n-1]\n",
    "    return f\n",
    "y1=y1()\n",
    "\n",
    "\n",
    "def y2():\n",
    "    mem=[np.pi**2/6]\n",
    "    def f(n):\n",
    "        while len(mem)<n:\n",
    "            mem.append(mem[-1]-1/len(mem)**2)\n",
    "        return mem[n-1]\n",
    "    return f\n",
    "y2=y2()\n",
    "\n",
    "\n",
    "DCResults={}\n",
    "\n",
    "counts=DCCounts\n",
    "DCResult=[]\n",
    "visited=set()\n",
    "# Recursive function like before, to explore in DFS the space of predicate sets up to a given depth.\n",
    "def search(preds,cols):\n",
    "    #Every candidate needs to be evaluated only once\n",
    "    if preds in visited:\n",
    "            return\n",
    "    visited.add(preds)    \n",
    "    \n",
    "    #Like before, stop at maximum depth\n",
    "    if len(preds)>=depth:\n",
    "            return\n",
    "    \n",
    "    #If we are evaluating {A,B,C}, we iterate through p(A|BC),p(B|AC),p(C|AB).\n",
    "    #For each of them, we compare it with the respective subsets:\n",
    "    # p(A|BC)< p(A|B), p(A|C), p(A)\n",
    "    # p(B|AC)< p(B|A), p(B|C), p(B)\n",
    "    # p(C|AB)< p(C|A), p(C|B), p(C)\n",
    "\n",
    "    #A DC is only accepted when the inequalities are statistically significant\n",
    "    # indicating there is some relationship among this particular set of predicates\n",
    "    # that is not captured by any of its subsets\n",
    "    for pred in ds.sortedPreds:\n",
    "            #As before, we do not add predicates over already used columns to avoid trivial DCs.\n",
    "            ncol=ds.predCols[pred]\n",
    "            if ncol in cols:\n",
    "                continue\n",
    "\n",
    "            #Determine if the DC is valid\n",
    "            npreds=preds|{pred}\n",
    "            ncols=cols|{ncol}\n",
    "\n",
    "            #a and b are number of successes and failures of a Bernouilli radnom variable\n",
    "            #In our case, the number of tuple pairs that fulfill pred when the others in preds are true.\n",
    "            # Ex: a1=|ABC|/|BC|, b1=|(!A)BC|/|BC|\n",
    "            a1=int(counts[npreds])\n",
    "            b1=int(counts[preds])-a1\n",
    "\n",
    "            #Assume validity until some conditional of a subset is not significantly different from the current conditional\n",
    "            valid=True\n",
    "            for subPreds in powerset(preds):\n",
    "                #For every subset of preds, compare the conditionals\n",
    "                #Ex: compare p(A|BC) with p(A|B)\n",
    "                subPreds=frozenset(subPreds)\n",
    "                npreds2=subPreds|{pred}\n",
    "\n",
    "                # Ex: a1=|AB|/|B|, b1=|(!A)B|/|B|\n",
    "                a2=int(counts[npreds2])\n",
    "                b2=int(counts[subPreds])-a2\n",
    "\n",
    "                #If the population probability of p(A|BC) is u1 and the one of p(A|B) is u2, \n",
    "                #given our information a1,a2,b1,b2, we know the distribution of ln((a1*b2)/(b1*a2)) has mean:                \n",
    "                u=y1(a1+1)-y1(b1+1)-y1(a2+1)+y1(b2+1)\n",
    "\n",
    "                #and standard deviation\n",
    "                s=np.sqrt(y2(a1+1)+y2(b1+1)+y2(a2+1)+y2(b2+1))\n",
    "\n",
    "                #If the mean is more than 2 standard deviations from 0, it statistically significant enough for us to claim\n",
    "                #the conditional p(A|BC) is smaller than p(A|B), and therefore there is some relationship\n",
    "                #Otherwise, there is not enough evidence and this difference in distribution could be due to randomness.\n",
    "                if u+2*s>0:\n",
    "                    valid=False\n",
    "                    break\n",
    "            if valid:\n",
    "                #If is valid, there is some significant relationship between the predicates\n",
    "                #However, we only accept a DC when it is a set of exclusive predicates.\n",
    "                #This means p(A|BC) must be 0.\n",
    "                #Optionally, approximate DCs may be discovered by changing this condition to:\n",
    "                # a1/(a1+b1)<0.01\n",
    "                # where 0.01 is a 1% approximation factor.\n",
    "                if a1==0:                                          \n",
    "                    DCResult.append((preds,pred))\n",
    "                else:\n",
    "                    search(npreds,ncols)\n",
    "                    \n",
    "#Begin search on the empty set of predicates\n",
    "search(frozenset(),frozenset())\n",
    "\n",
    "DCResults=DCResult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This final cell of code exports the valid DCs to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T14:26:29.463203Z",
     "start_time": "2025-02-27T14:26:29.451844Z"
    }
   },
   "outputs": [],
   "source": [
    "def getPred(i,ds):\n",
    "    return (ds.preds[i])\n",
    "\n",
    "\n",
    "dcs=set({})\n",
    "\n",
    "with open(f\"result_{dataset}\",\"w\") as f:\n",
    "    for dc in [ DenialConstraint([ getPred(p,ds) for p in preds]+[getPred(pred,ds)]) for preds,pred in DCResults]:\n",
    "        s=frozenset(dc.preds)\n",
    "        if s not in dcs:\n",
    "            dcs.add(s)\n",
    "            f.write(dc.__repr__()+\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
